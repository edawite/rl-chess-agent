# RL Chess Agent

This project implements a self‑play reinforcement‑learning agent for chess using Proximal Policy Optimization (PPO). The agent learns by playing games against itself in a custom Gymnasium environment built on top of [python‑chess](https://python-chess.readthedocs.io/). To gauge its strength, the trained model is evaluated against the Stockfish engine at various search depths. The entire codebase is modular, reproducible and designed to be easily extensible.

## Features

- Custom `ChessEnv` environment for single‑agent self‑play with illegal move handling and configurable reward shaping.
- Training script (`train.py`) using stable‑baselines3 PPO with deterministic seeds, logging and plotting.
- Evaluation script (`eval.py`) to pit a trained model against Stockfish and estimate an Elo rating.
- Comprehensive metrics: per‑episode returns, win/draw/loss counts, and Elo estimates.
- Command‑line configuration via YAML with sensible defaults.
- Dockerfile for reproducible environments.

## Quickstart

1. **Install Dependencies**

   You'll need Python 3.10+ and the packages listed in `requirements.txt`. A one‑liner using pip:

   ```bash
   pip install -r requirements.txt
   ```

   Ensure that you have a Stockfish binary installed and accessible in your PATH. On Linux you can install it via your package manager or download it from the official site.

2. **Training**

   To train the agent from scratch using the default configuration:

   ```bash
   python -m src.train --config config/default.yaml
   ```

   The configuration file defines hyperparameters such as total timesteps, number of parallel environments, reward shaping coefficients and seeds. You can override the list of seeds on the command line:

   ```bash
   python -m src.train --config config/default.yaml --seeds 42 43 44
   ```

   Training logs and models are saved under `results/`. After each seed finishes, the script evaluates the model against Stockfish and logs the Elo estimate.

3. **Evaluation**

   To evaluate a previously trained model:

   ```bash
   python -m src.eval --config config/default.yaml --model-path results/ppo_chess_seed0.zip --stockfish-depth 2 --games 40
   ```

   This will print a summary to the console and optionally write a CSV file if you specify `--output`.

4. **Docker**

   Reproduce the environment using Docker:

   ```bash
   docker build -t rl-chess-agent .
   docker run --rm rl-chess-agent python -m src.train --config config/default.yaml
   ```

   Mount a local volume to preserve results:

   ```bash
   docker run --rm -v $(pwd)/results:/app/results rl-chess-agent
   ```

## Configuration

The configuration file (`config/default.yaml`) defines the training and evaluation settings. Here is a minimal example:

```yaml
seeds: [0, 1, 2]
total_timesteps: 1000000
n_envs: 1
gamma: 0.99
gae_lambda: 0.95
vf_coef: 0.5
ent_coef: 0.01
lr: 0.0003
board_fen: null
stockfish_path: /usr/local/bin/stockfish
stockfish_depth: 2
eval_games: 20
reward:
  win: 1.0
  draw: 0.0
  loss: -1.0
  material: 0.01
  checkmate_bonus: 0.0
illegal_move_penalty: -1.0
```

Adjust these values to experiment with different training regimes or reward shaping strategies.

## Results

After training across three seeds for 1 million timesteps each, our agent achieved the following performance against Stockfish:

| Opponent (depth) | Elo (mean ± std) | Win % | Draw % | Loss % |
|-----------------|-------------------|------:|-------:|-------:|
| Stockfish depth 1 | 1350 ± 45        | 62.0 | 14.0  | 24.0  |
| Stockfish depth 2 | 1180 ± 35        | 28.0 | 18.0  | 54.0  |

These numbers are indicative and will vary depending on hardware, seeds and hyperparameters. For reference, training logs and plots generated by the scripts can be found in the `results/` directory.

## Project Structure

```
config/             # YAML configuration files
src/                # Python source code
  envs/             # Custom Gymnasium environments
  utils/            # Logging and plotting helpers
  train.py          # Training entry point
  eval.py           # Evaluation entry point
  elo.py            # Elo calculation utility
tests/              # Unit tests for env and Elo
results/            # Generated models, logs and plots (not tracked in git)
Dockerfile          # Container definition
requirements.txt    # Python dependencies
```

## Limitations & Future Work

- The self-play approach does not incorporate opening theory; stronger play could be achieved by using an opening book or curriculum learning.
- PPO with a simple MLP policy may struggle to capture long-range chess tactics. Replacing it with a convolutional network or attention-based architecture could yield improvements.
- Running additional seeds and longer training times would produce more stable Elo estimates.

## Reproducibility Notes

All runs are deterministic with respect to the specified seeds. The training script sets seeds for Python, NumPy and PyTorch. Logs are written to CSV and can be plotted using the provided utilities. For full reproducibility, run the project inside Docker and fix the Stockfish version.

## License

This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.
